/**
 * ğŸŸ¢ TDD ç»¿é˜¶æ®µï¼šMCP OpenAI é›†æˆåŠŸèƒ½å®ç°
 * ä¸ OpenAI API é›†æˆï¼Œæ”¯æŒå·¥å…·è°ƒç”¨å’Œæç¤ºè¯
 */

import { OpenAI } from 'openai';
import * as dotenv from 'dotenv';
import { MCPServer } from './mcp-server';

// åŠ è½½ç¯å¢ƒå˜é‡
dotenv.config({ debug: false, quiet: true });

/**
 * OpenAI å®¢æˆ·ç«¯é…ç½®æ¥å£
 */
export interface MCPOpenAIConfig {
  /** OpenAI API å¯†é’¥ */
  apiKey: string;
  /** ä½¿ç”¨çš„æ¨¡å‹åç§° */
  model: string;
  /** API åŸºç¡€ URL */
  baseURL?: string;
  /** æœ€å¤§ä»¤ç‰Œæ•° */
  maxTokens?: number;
  /** æ¸©åº¦å‚æ•° */
  temperature?: number;
  /** è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ */
  timeout?: number;
}

/**
 * OpenAI å·¥å…·è°ƒç”¨æ ¼å¼
 */
export interface OpenAITool {
  type: 'function';
  function: {
    name: string;
    description: string;
    parameters: Record<string, any>;
  };
}

/**
 * OpenAI å·¥å…·è°ƒç”¨ç»“æœ
 */
export interface OpenAIToolCallResult {
  tool_call_id: string;
  role: 'tool';
  content: string;
}

/**
 * MCP OpenAI å®¢æˆ·ç«¯
 * å°† MCP æœåŠ¡å™¨ä¸ OpenAI API é›†æˆ
 */
export class MCPOpenAIClient {
  private openai: OpenAI;
  private config: MCPOpenAIConfig;
  private mcpServer: MCPServer;

  constructor(config: MCPOpenAIConfig, mcpServer: MCPServer) {
    this.config = config;
    this.mcpServer = mcpServer;

    // åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
    this.openai = new OpenAI({
      apiKey: config.apiKey,
      baseURL: config.baseURL,
      timeout: config.timeout || 30000
    });
  }

  /**
   * ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
   */
  static loadConfigFromEnv(): MCPOpenAIConfig {
    const apiKey = process.env['OPENAI_API_KEY'];
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®');
    }

    const config: MCPOpenAIConfig = {
      apiKey,
      model: process.env['OPENAI_MODEL'] || 'gpt-4'
    };

    if (process.env['OPENAI_BASE_URL']) {
      config.baseURL = process.env['OPENAI_BASE_URL'];
    }
    if (process.env['OPENAI_MAX_TOKENS']) {
      config.maxTokens = parseInt(process.env['OPENAI_MAX_TOKENS']);
    }
    if (process.env['OPENAI_TEMPERATURE']) {
      config.temperature = parseFloat(process.env['OPENAI_TEMPERATURE']);
    }

    return config;
  }

  /**
   * è·å–é…ç½®ä¿¡æ¯
   */
  getConfig(): MCPOpenAIConfig {
    return { ...this.config };
  }

  /**
   * è®¾ç½® MCP æœåŠ¡å™¨
   */
  setMCPServer(server: MCPServer): void {
    this.mcpServer = server;
  }

  /**
   * è·å– MCP æœåŠ¡å™¨
   */
  getMCPServer(): MCPServer {
    return this.mcpServer;
  }

  /**
   * å°† MCP å·¥å…·è½¬æ¢ä¸º OpenAI å‡½æ•°æ ¼å¼
   */
  getOpenAITools(): OpenAITool[] {
    const mcpTools = this.mcpServer.getTools();
    return mcpTools.map(tool => ({
      type: 'function' as const,
      function: {
        name: tool.name,
        description: tool.description,
        parameters: tool.schema || {}
      }
    }));
  }

  /**
   * æ‰§è¡Œå·¥å…·è°ƒç”¨
   */
  async executeToolCall(toolCall: OpenAI.Chat.Completions.ChatCompletionMessageToolCall): Promise<OpenAIToolCallResult> {
    try {
      if (toolCall.type !== 'function') {
        throw new Error(`ä¸æ”¯æŒçš„å·¥å…·è°ƒç”¨ç±»å‹: ${toolCall.type}`);
      }

      const functionName = toolCall.function.name;
      const functionArgs = JSON.parse(toolCall.function.arguments);

      const result = await this.mcpServer.executeTool(functionName, functionArgs);

      return {
        tool_call_id: toolCall.id,
        role: 'tool',
        content: JSON.stringify(result)
      };
    } catch (error) {
      return {
        tool_call_id: toolCall.id,
        role: 'tool',
        content: JSON.stringify({
          error: error instanceof Error ? error.message : 'å·¥å…·æ‰§è¡Œå¤±è´¥'
        })
      };
    }
  }

  /**
   * èŠå¤©å®Œæˆ
   */
  async chatCompletion(
    messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[],
    options?: Partial<OpenAI.Chat.Completions.ChatCompletionCreateParams>
  ): Promise<OpenAI.Chat.Completions.ChatCompletion> {
    const params: any = {
      model: this.config.model,
      messages,
      stream: false,
      ...options
    };

    if (this.config.maxTokens !== undefined) {
      params.max_tokens = this.config.maxTokens;
    }
    if (this.config.temperature !== undefined) {
      params.temperature = this.config.temperature;
    }

    return await this.openai.chat.completions.create(params) as OpenAI.Chat.Completions.ChatCompletion;
  }

  /**
   * å¸¦å·¥å…·è°ƒç”¨çš„èŠå¤©å®Œæˆ
   */
  async chatCompletionWithTools(
    messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[],
    options?: Partial<OpenAI.Chat.Completions.ChatCompletionCreateParams>
  ): Promise<OpenAI.Chat.Completions.ChatCompletion> {
    const tools = this.getOpenAITools();

    const params: any = {
      model: this.config.model,
      messages,
      stream: false,
      ...options
    };

    if (tools.length > 0) {
      params.tools = tools;
      params.tool_choice = 'auto';
    }
    if (this.config.maxTokens !== undefined) {
      params.max_tokens = this.config.maxTokens;
    }
    if (this.config.temperature !== undefined) {
      params.temperature = this.config.temperature;
    }

    return await this.openai.chat.completions.create(params) as OpenAI.Chat.Completions.ChatCompletion;
  }

  /**
   * ä½¿ç”¨æç¤ºè¯è¿›è¡ŒèŠå¤©å®Œæˆ
   */
  async chatCompletionWithPrompt(
    promptName: string,
    promptArgs: Record<string, any>,
    messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[],
    options?: Partial<OpenAI.Chat.Completions.ChatCompletionCreateParams>
  ): Promise<OpenAI.Chat.Completions.ChatCompletion> {
    const promptManager = this.mcpServer.getPromptManager();
    if (!promptManager) {
      throw new Error('Prompt ç®¡ç†å™¨æœªè®¾ç½®');
    }

    const renderedPrompt = await promptManager.renderPrompt(promptName, promptArgs);

    // å°†æ¸²æŸ“åçš„æç¤ºè¯æ·»åŠ åˆ°æ¶ˆæ¯å¼€å¤´
    const systemMessage: OpenAI.Chat.Completions.ChatCompletionMessageParam = {
      role: 'system',
      content: renderedPrompt
    };

    const allMessages = [systemMessage, ...messages];

    return await this.chatCompletionWithTools(allMessages, options);
  }

  /**
   * æµå¼èŠå¤©å®Œæˆ
   */
  async chatCompletionStream(
    messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[],
    options?: Partial<OpenAI.Chat.Completions.ChatCompletionCreateParams>
  ): Promise<AsyncIterable<OpenAI.Chat.Completions.ChatCompletionChunk>> {
    const params: any = {
      model: this.config.model,
      messages,
      stream: true,
      ...options
    };

    if (this.config.maxTokens !== undefined) {
      params.max_tokens = this.config.maxTokens;
    }
    if (this.config.temperature !== undefined) {
      params.temperature = this.config.temperature;
    }

    return await this.openai.chat.completions.create(params) as any;
  }

  /**
   * å®Œæ•´çš„å¯¹è¯å¤„ç†ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ï¼‰
   */
  async processConversation(
    messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[],
    maxIterations: number = 5
  ): Promise<{
    messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[];
    finalResponse: OpenAI.Chat.Completions.ChatCompletion;
    toolCallsExecuted: number;
  }> {
    const conversationMessages = [...messages];
    let toolCallsExecuted = 0;
    let finalResponse: OpenAI.Chat.Completions.ChatCompletion;

    for (let i = 0; i < maxIterations; i++) {
      const response = await this.chatCompletionWithTools(conversationMessages);
      finalResponse = response;

      const assistantMessage = response.choices[0]?.message;
      if (!assistantMessage) break;

      conversationMessages.push(assistantMessage);

      // æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
      if (assistantMessage.tool_calls && assistantMessage.tool_calls.length > 0) {
        // æ‰§è¡Œæ‰€æœ‰å·¥å…·è°ƒç”¨
        for (const toolCall of assistantMessage.tool_calls) {
          const toolResult = await this.executeToolCall(toolCall);
          conversationMessages.push(toolResult);
          toolCallsExecuted++;
        }
      } else {
        // æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œå¯¹è¯ç»“æŸ
        break;
      }
    }

    return {
      messages: conversationMessages,
      finalResponse: finalResponse!,
      toolCallsExecuted
    };
  }
}
