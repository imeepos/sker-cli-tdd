/**
 * ğŸŸ¢ TDD ç»¿é˜¶æ®µï¼šCLI å·¥å…·æœ€å°å®ç°
 * å‘½ä»¤è¡Œå·¥å…·æ ¸å¿ƒåŠŸèƒ½
 */

import { MCPOpenAIClient, MCPOpenAIConfig } from './mcp-openai';

/**
 * CLI é…ç½®æ¥å£
 */
export interface CLIConfig extends MCPOpenAIConfig {
  stream?: boolean;
  interactive?: boolean;
}

/**
 * å‘½ä»¤è¡Œé€‰é¡¹æ¥å£
 */
export interface CLIOptions {
  model?: string;
  temperature?: number;
  maxTokens?: number;
  stream?: boolean;
  interactive?: boolean;
  help?: boolean;
  version?: boolean;
}

/**
 * CLI å·¥å…·ç±»
 */
export class CLI {
  private openaiClient?: MCPOpenAIClient;

  /**
   * è®¾ç½® OpenAI å®¢æˆ·ç«¯
   */
  setOpenAIClient(client: MCPOpenAIClient): void {
    this.openaiClient = client;
  }

  /**
   * è·å– OpenAI å®¢æˆ·ç«¯
   */
  getOpenAIClient(): MCPOpenAIClient | undefined {
    return this.openaiClient;
  }

  /**
   * è·å–é»˜è®¤é…ç½®
   */
  getDefaultConfig(): CLIConfig {
    return {
      apiKey: '',
      model: 'gpt-4',
      temperature: 0.7,
      maxTokens: 2000,
      stream: true,
      interactive: false
    };
  }

  /**
   * ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½®
   */
  loadConfigFromEnv(): CLIConfig {
    const apiKey = process.env['OPENAI_API_KEY'];
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®');
    }

    const config: CLIConfig = {
      apiKey,
      model: process.env['OPENAI_MODEL'] || 'gpt-4',
      temperature: process.env['OPENAI_TEMPERATURE'] ? parseFloat(process.env['OPENAI_TEMPERATURE']) : 0.7,
      maxTokens: process.env['OPENAI_MAX_TOKENS'] ? parseInt(process.env['OPENAI_MAX_TOKENS']) : 2000
    };

    if (process.env['OPENAI_BASE_URL']) {
      config.baseURL = process.env['OPENAI_BASE_URL'];
    }

    return config;
  }

  /**
   * è§£æå‘½ä»¤è¡Œå‚æ•°
   */
  parseArgs(args: string[]): CLIOptions {
    const options: CLIOptions = {};

    for (let i = 0; i < args.length; i++) {
      const arg = args[i];
      
      switch (arg) {
        case '--model':
          options.model = args[++i];
          break;
        case '--temperature':
          const tempValue = args[++i];
          if (tempValue) options.temperature = parseFloat(tempValue);
          break;
        case '--max-tokens':
          const tokensValue = args[++i];
          if (tokensValue) options.maxTokens = parseInt(tokensValue);
          break;
        case '--stream':
          options.stream = true;
          break;
        case '--interactive':
          options.interactive = true;
          break;
        case '--help':
        case '-h':
          options.help = true;
          break;
        case '--version':
        case '-v':
          options.version = true;
          break;
      }
    }

    return options;
  }

  /**
   * æµå¼èŠå¤©
   */
  async streamChat(message: string): Promise<string> {
    if (!this.openaiClient) {
      throw new Error('OpenAI å®¢æˆ·ç«¯æœªè®¾ç½®');
    }

    const messages = [{ role: 'user' as const, content: message }];
    const stream = await this.openaiClient.chatCompletionStream(messages);

    let fullResponse = '';
    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content;
      if (content) {
        process.stdout.write(content);
        fullResponse += content;
      }
    }

    return fullResponse;
  }

  /**
   * è·å–å¯ç”¨å·¥å…·åˆ—è¡¨
   */
  getAvailableTools(): any[] {
    if (!this.openaiClient) {
      return [];
    }
    return this.openaiClient.getOpenAITools();
  }

  /**
   * å¸¦å·¥å…·è°ƒç”¨çš„èŠå¤©
   */
  async chatWithTools(message: string): Promise<any> {
    if (!this.openaiClient) {
      throw new Error('OpenAI å®¢æˆ·ç«¯æœªè®¾ç½®');
    }

    const messages = [{ role: 'user' as const, content: message }];
    const response = await this.openaiClient.chatCompletionWithTools(messages);

    // å¤„ç†å·¥å…·è°ƒç”¨
    const assistantMessage = response.choices[0]?.message;
    if (assistantMessage?.tool_calls && assistantMessage.tool_calls.length > 0) {
      for (const toolCall of assistantMessage.tool_calls) {
        await this.openaiClient.executeToolCall(toolCall);
      }
    }

    return response;
  }

  /**
   * å¯åŠ¨äº¤äº’å¼æ¨¡å¼
   */
  async startInteractiveMode(): Promise<void> {
    const inquirer = require('inquirer');
    
    console.log('ğŸ¤– è¿›å…¥äº¤äº’å¼èŠå¤©æ¨¡å¼ (è¾“å…¥ /exit é€€å‡º)');
    
    while (true) {
      const { message } = await inquirer.prompt([
        {
          type: 'input',
          name: 'message',
          message: 'ä½ :'
        }
      ]);

      if (message === '/exit') {
        console.log('ğŸ‘‹ å†è§ï¼');
        break;
      }

      try {
        await this.streamChat(message);
        console.log('\n');
      } catch (error) {
        console.error('âŒ é”™è¯¯:', (error as Error).message);
      }
    }
  }

  /**
   * è·å–å¸®åŠ©æ–‡æœ¬
   */
  getHelpText(): string {
    return `
ğŸ¤– MCP OpenAI CLI å·¥å…·

ä½¿ç”¨æ–¹æ³•:
  sker-cli [é€‰é¡¹] [æ¶ˆæ¯]

é€‰é¡¹:
  --model <model>           æŒ‡å®šæ¨¡å‹ (é»˜è®¤: gpt-4)
  --temperature <temp>      è®¾ç½®æ¸©åº¦å‚æ•° (0-1)
  --max-tokens <tokens>     æœ€å¤§ä»¤ç‰Œæ•°
  --stream                  å¯ç”¨æµå¼è¾“å‡º
  --interactive, -i         äº¤äº’å¼æ¨¡å¼
  --help, -h               æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
  --version, -v            æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯

ç¤ºä¾‹:
  sker-cli "ä½ å¥½ï¼Œä¸–ç•Œï¼"
  sker-cli --interactive
  sker-cli --model gpt-3.5-turbo --stream "è§£é‡Šé‡å­è®¡ç®—"
`;
  }

  /**
   * è·å–ç‰ˆæœ¬ä¿¡æ¯
   */
  getVersion(): string {
    return '1.0.0';
  }

  /**
   * éªŒè¯é…ç½®
   */
  validateConfig(config: CLIConfig): void {
    if (!config.apiKey) {
      throw new Error('é…ç½®æ— æ•ˆ: ç¼ºå°‘ API å¯†é’¥');
    }
    if (!config.model) {
      throw new Error('é…ç½®æ— æ•ˆ: ç¼ºå°‘æ¨¡å‹åç§°');
    }
  }

  /**
   * é”™è¯¯å¤„ç†
   */
  async handleError(error: Error): Promise<void> {
    console.error(`âŒ é”™è¯¯: ${error.message}`);
  }
}
